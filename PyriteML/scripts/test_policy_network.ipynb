{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Workspace, policy, and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "\n",
    "# # use line-buffering for both stdout and stderr\n",
    "# sys.stdout = open(sys.stdout.fileno(), mode='w', buffering=1)\n",
    "# sys.stderr = open(sys.stderr.fileno(), mode='w', buffering=1)\n",
    "\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.dataset.base_dataset import BaseImageDataset, BaseDataset\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "\n",
    "# allows arbitrary python code execution in configs using the ${eval:''} resolver\n",
    "OmegaConf.register_new_resolver(\"eval\", eval, replace=True)\n",
    "\n",
    "temp_output_dir = \"./temp_output_dir\"\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "with initialize(\n",
    "    version_base=None,\n",
    "    config_path=str('../diffusion_policy/config'),\n",
    "    job_name=\"test_app\"\n",
    "):\n",
    "    print(\"Test Script: starting.\")\n",
    "    cfg = compose(config_name=\"train_diffusion_unet_timm_flip_up_workspace\")\n",
    "    # resolve immediately so all the ${now:} resolvers\n",
    "    # will use the same time.\n",
    "    print(\"Test Script: resolving config.\")\n",
    "    OmegaConf.resolve(cfg)\n",
    "\n",
    "    print(\"Test Script: initializing workspace.\")\n",
    "    cls = hydra.utils.get_class(cfg._target_)\n",
    "    workspace: BaseWorkspace = cls(cfg)\n",
    "    policy = workspace.model\n",
    "\n",
    "    # configure dataset\n",
    "    print(\"Test Script: configuring dataset.\")\n",
    "    dataset: BaseImageDataset\n",
    "    dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "    assert isinstance(dataset, BaseImageDataset) or isinstance(dataset, BaseDataset)\n",
    "    print(\"Test Script: Creating dataloader.\")\n",
    "    train_dataloader = DataLoader(dataset, **cfg.dataloader)\n",
    "    print('train dataset:', len(dataset), 'train dataloader:', len(train_dataloader))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset[5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute normalizer on the main process and save to disk\n",
    "print(\"Test Script: Computing normalizer.\")\n",
    "sparse_normalizer_path = os.path.join(temp_output_dir, 'sparse_normalizer.pkl')\n",
    "dense_normalizer_path = os.path.join(temp_output_dir, 'dense_normalizer.pkl')\n",
    "sparse_normalizer, dense_normalizer = dataset.get_normalizer()\n",
    "policy.set_normalizer(sparse_normalizer, dense_normalizer)\n",
    "\n",
    "\n",
    "device = policy.device\n",
    "print(\"Test Script: done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweep dataloader parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sweep batch size and num_workers\n",
    "# import time\n",
    "# from tqdm import trange, tqdm\n",
    "\n",
    "\n",
    "# batch_sizes = [16, 32, 64, 128, 256]\n",
    "# num_workers = [4,8,16,32]\n",
    "# # batch_sizes = [64]\n",
    "# # num_workers = [32]\n",
    "# timings = {}\n",
    "\n",
    "# # 5.5 seconds is the threshold for the time it takes to load 10% of the dataset\n",
    "# time_threshold = 20 + 10\n",
    "# check_ratio = 0.3\n",
    "# device = 'cuda:0'\n",
    "# print('starting')\n",
    "# print('device:', device)\n",
    "# for batch_size in batch_sizes:\n",
    "#     for num_worker in num_workers:\n",
    "#         dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_worker)\n",
    "\n",
    "#         start_time = time.time()\n",
    "#         # dataset.get_normalizer(batch_size, num_worker)\n",
    "#         finished = True\n",
    "#         with tqdm(dataloader, desc=f'iterating dataset with batch_size={batch_size}, num_workers={num_worker}.') as tepoch:\n",
    "#             for batch_idx, batch in enumerate(tepoch):\n",
    "#                 batch = dict_apply(batch, lambda x: x.to(device, non_blocking=True))\n",
    "#                 if time.time() - start_time > time_threshold:\n",
    "#                     if batch_idx / len(tepoch) < check_ratio:\n",
    "#                         print(f\"Time exceeded threshold of {time_threshold} seconds, but data loaded is less than {check_ratio}. Exiting.\")\n",
    "#                         finished = False\n",
    "#                         break\n",
    "#         end_time = time.time()\n",
    "#         elapsed_time = end_time - start_time\n",
    "#         if finished:\n",
    "#             timings[(batch_size, num_worker)] = elapsed_time\n",
    "\n",
    "# fastest_batch_size = min(timings, key=timings.get)\n",
    "# print(\"Fastest batch size:\", fastest_batch_size, \", Fastest time:\", timings[fastest_batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read a batch of data, test normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reach one batch of data from the dataloader\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "print(batch.keys())\n",
    "for key, attr in batch['obs']['sparse'].items():\n",
    "    print(\"   obs.sparse.key: \", key, attr.shape)\n",
    "for key, attr in batch['obs']['dense'].items():\n",
    "    print(\"   obs.dense.key: \", key, attr.shape)\n",
    "\n",
    "nobs_sparse = sparse_normalizer.normalize(batch['obs']['sparse'])\n",
    "nactions_sparse = sparse_normalizer['action'].normalize(batch['action']['sparse'])\n",
    "# nactions_dense = dense_normalizer['action'].normalize(batch['action']['dense'])\n",
    "\n",
    "\n",
    "\n",
    "# test normalizer\n",
    "print(\"policy debug: batch['action']['sparse'][0,0,:]: \", batch['action']['sparse'][0,0,:])\n",
    "print(\"policy debug: nactions_sparse[0,0,:]: \", nactions_sparse[0,0,:])\n",
    "\n",
    "# batch = dict_apply(batch, lambda x: x.to(device, non_blocking=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test compute_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# flag = {\n",
    "#     'start_training_dense': True,\n",
    "#     'dense_traj_cond_use_gt': True\n",
    "# }\n",
    "# raw_loss = policy(batch, flag)\n",
    "# print(raw_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# def log_action_mse(step_log, category, pred_action, gt_action):\n",
    "#     B, T, _ = pred_action['sparse'].shape\n",
    "#     pred_action_sparse = pred_action['sparse'].view(B, T, -1, 9)\n",
    "#     gt_action_sparse = gt_action['sparse'].view(B, T, -1, 9)\n",
    "#     step_log[f'{category}_sparse_action_mse_error'] = torch.nn.functional.mse_loss(pred_action_sparse, gt_action_sparse)\n",
    "#     step_log[f'{category}_sparse_action_mse_error_pos'] = torch.nn.functional.mse_loss(pred_action_sparse[..., :3], gt_action_sparse[..., :3])\n",
    "#     step_log[f'{category}_sparse_action_mse_error_rot'] = torch.nn.functional.mse_loss(pred_action_sparse[..., 3:9], gt_action_sparse[..., 3:9])\n",
    "#     # step_log[f'{category}_sparse_action_mse_error_width'] = torch.nn.functional.mse_loss(pred_action_sparse[..., 9], gt_action_sparse[..., 9])\n",
    "#     B, T, _, _= pred_action['dense'].shape\n",
    "#     pred_action_dense = pred_action['dense'].view(B, T, -1, 9)\n",
    "#     gt_action_dense = gt_action['dense'].view(B, T, -1, 9)\n",
    "#     step_log[f'{category}_dense_action_mse_error'] = torch.nn.functional.mse_loss(pred_action_dense, gt_action_dense)\n",
    "#     step_log[f'{category}_dense_action_mse_error_pos'] = torch.nn.functional.mse_loss(pred_action_dense[..., :3], gt_action_dense[..., :3])\n",
    "#     step_log[f'{category}_dense_action_mse_error_rot'] = torch.nn.functional.mse_loss(pred_action_dense[..., 3:9], gt_action_dense[..., 3:9])\n",
    "#     # step_log[f'{category}_dense_action_mse_error_width'] = torch.nn.functional.mse_loss(pred_action_dense[..., 9], gt_action_dense[..., 9])\n",
    "                \n",
    "# # sample trajectory from training set, and evaluate difference\n",
    "# gt_action = batch['action']\n",
    "# pred_action = policy.predict_action(batch['obs'])\n",
    "# print(\"gt_action['sparse'].shape: \", gt_action['sparse'].shape)\n",
    "# print(\"pred_action['sparse'].shape: \", pred_action['sparse'].shape)\n",
    "# print(\"gt_action['dense'].shape: \", gt_action['dense'].shape)\n",
    "# print(\"pred_action['dense'].shape: \", pred_action['dense'].shape)\n",
    "\n",
    "# step_log = {}\n",
    "# log_action_mse(step_log, 'train', pred_action, gt_action)\n",
    "# print(step_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
