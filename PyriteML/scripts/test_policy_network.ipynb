{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Workspace, policy, and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/robotlab/ACP/adaptive_compliance_policy/PyriteML\n",
      "Test Script: starting.\n",
      "Test Script: resolving config.\n",
      "Test Script: initializing workspace.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robotlab/micromamba/envs/pyrite/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/robotlab/micromamba/envs/pyrite/lib/python3.12/site-packages/torch/nn/init.py:566: UserWarning:\n",
      "\n",
      "Initializing zero-element tensors is a no-op\n",
      "\n",
      "vit will use the CLS token. feature_aggregation (attention_pool_2d) is ignored!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb keys:          ['rgb_0']\n",
      "low_dim_keys keys: ['robot0_eef_pos', 'robot0_eef_rot_axis_angle']\n",
      "==> reduce pretrained obs_encorder's lr\n",
      "==> rgb keys:  ['rgb_0']\n",
      "obs_encorder params: 151\n",
      "Test Script: configuring dataset.\n",
      "[VirtualTargetDataset] loading data into store\n",
      "[ReplayBuffer] checking chunk size and compressor.\n",
      " checking:  episode_1727235551\n",
      " checking:  episode_1727235569\n",
      " checking:  episode_1727235583\n",
      " checking:  episode_1727235597\n",
      " checking:  episode_1727235613\n",
      " checking:  episode_1727235630\n",
      " checking:  episode_1727235645\n",
      " checking:  episode_1727235661\n",
      " checking:  episode_1727235681\n",
      " checking:  episode_1727235696\n",
      " checking:  episode_1727235710\n",
      " checking:  episode_1727235725\n",
      " checking:  episode_1727235744\n",
      " checking:  episode_1727235765\n",
      " checking:  episode_1727235779\n",
      " checking:  episode_1727235795\n",
      " checking:  episode_1727235810\n",
      " checking:  episode_1727235825\n",
      " checking:  episode_1727235839\n",
      " checking:  episode_1727236158\n",
      " checking:  episode_1727236172\n",
      " checking:  episode_1727236186\n",
      " checking:  episode_1727236202\n",
      " checking:  episode_1727236216\n",
      " checking:  episode_1727236238\n",
      " checking:  episode_1727236253\n",
      " checking:  episode_1727236265\n",
      " checking:  episode_1727236280\n",
      " checking:  episode_1727236293\n",
      " checking:  episode_1727236306\n",
      " checking:  episode_1727236422\n",
      " checking:  episode_1727236437\n",
      " checking:  episode_1727236453\n",
      " checking:  episode_1727236470\n",
      " checking:  episode_1727236484\n",
      " checking:  episode_1727236501\n",
      " checking:  episode_1727236518\n",
      " checking:  episode_1727236530\n",
      " checking:  episode_1727236543\n",
      " checking:  episode_1727236557\n",
      " checking:  episode_1727236577\n",
      " checking:  episode_1727236590\n",
      " checking:  episode_1727236603\n",
      " checking:  episode_1727236622\n",
      " checking:  episode_1727236635\n",
      " checking:  episode_1727236651\n",
      " checking:  episode_1727236663\n",
      " checking:  episode_1727236709\n",
      " checking:  episode_1727236724\n",
      " checking:  episode_1727236737\n",
      " checking:  episode_1727236774\n",
      " checking:  episode_1727236787\n",
      " checking:  episode_1727236800\n",
      " checking:  episode_1727236812\n",
      " checking:  episode_1727236825\n",
      " checking:  episode_1727236841\n",
      " checking:  episode_1727236854\n",
      " checking:  episode_1727236867\n",
      " checking:  episode_1727236878\n",
      " checking:  episode_1727236892\n",
      " checking:  episode_1727236906\n",
      " checking:  episode_1727236918\n",
      " checking:  episode_1727236930\n",
      " checking:  episode_1727236942\n",
      " checking:  episode_1727236957\n",
      " checking:  episode_1727236970\n",
      " checking:  episode_1727236984\n",
      " checking:  episode_1727236998\n",
      " checking:  episode_1727237012\n",
      " checking:  episode_1727237025\n",
      " checking:  episode_1727237040\n",
      " checking:  episode_1727237053\n",
      " checking:  episode_1727237065\n",
      " checking:  episode_1727237083\n",
      " checking:  episode_1727237119\n",
      " checking:  episode_1727237131\n",
      " checking:  episode_1727237142\n",
      " checking:  episode_1727237156\n",
      " checking:  episode_1727237166\n",
      " checking:  episode_1727237179\n",
      " checking:  episode_1727237196\n",
      " checking:  episode_1727237213\n",
      " checking:  episode_1727237225\n",
      " checking:  episode_1727237242\n",
      " checking:  episode_1727237253\n",
      " checking:  episode_1727237265\n",
      " checking:  episode_1727237282\n",
      " checking:  episode_1727237292\n",
      " checking:  episode_1727237304\n",
      " checking:  episode_1727237318\n",
      " checking:  episode_1727237402\n",
      " checking:  episode_1727237414\n",
      " checking:  episode_1727237432\n",
      " checking:  episode_1727237445\n",
      " checking:  episode_1727237458\n",
      " checking:  episode_1727237490\n",
      " checking:  episode_1727237505\n",
      " checking:  episode_1727237523\n",
      " checking:  episode_1727237539\n",
      " checking:  episode_1727237550\n",
      " checking:  episode_1727237561\n",
      " checking:  episode_1727237574\n",
      " checking:  episode_1727237590\n",
      " checking:  episode_1727237602\n",
      " checking:  episode_1727237621\n",
      " checking:  episode_1727237631\n",
      " checking:  episode_1727237642\n",
      " checking:  episode_1727237654\n",
      " checking:  episode_1727237666\n",
      " checking:  episode_1727237681\n",
      " checking:  episode_1727237695\n",
      " checking:  episode_1727237709\n",
      " checking:  episode_1727237723\n",
      " checking:  episode_1727237744\n",
      " checking:  episode_1727237757\n",
      " checking:  episode_1727237774\n",
      " checking:  episode_1727237786\n",
      " checking:  episode_1727237797\n",
      " checking:  episode_1727237815\n",
      " checking:  episode_1727237826\n",
      " checking:  episode_1727237841\n",
      " checking:  episode_1727237853\n",
      " checking:  episode_1727237873\n",
      " checking:  episode_1727237886\n",
      " checking:  episode_1727237900\n",
      " checking:  episode_1727237913\n",
      " checking:  episode_1727237928\n",
      " checking:  episode_1727237942\n",
      " checking:  episode_1727237956\n",
      " checking:  episode_1727237991\n",
      " checking:  episode_1727294154\n",
      " checking:  episode_1727294262\n",
      " checking:  episode_1727294278\n",
      " checking:  episode_1727294290\n",
      " checking:  episode_1727294303\n",
      " checking:  episode_1727294314\n",
      " checking:  episode_1727294326\n",
      " checking:  episode_1727294341\n",
      " checking:  episode_1727294351\n",
      " checking:  episode_1727294366\n",
      " checking:  episode_1727294378\n",
      " checking:  episode_1727294389\n",
      " checking:  episode_1727294407\n",
      " checking:  episode_1727294417\n",
      " checking:  episode_1727294432\n",
      " checking:  episode_1727294464\n",
      " checking:  episode_1727294482\n",
      " checking:  episode_1727294501\n",
      " checking:  episode_1727294515\n",
      " checking:  episode_1727294531\n",
      " checking:  episode_1727294545\n",
      " checking:  episode_1727294558\n",
      " checking:  episode_1727294573\n",
      " checking:  episode_1727294596\n",
      " checking:  episode_1727294610\n",
      " checking:  episode_1727294624\n",
      " checking:  episode_1727294639\n",
      " checking:  episode_1727294652\n",
      " checking:  episode_1727294668\n",
      " checking:  episode_1727294683\n",
      " checking:  episode_1727294696\n",
      " checking:  episode_1727294709\n",
      " checking:  episode_1727294749\n",
      " checking:  episode_1727294765\n",
      " checking:  episode_1727294782\n",
      " checking:  episode_1727294804\n",
      " checking:  episode_1727294817\n",
      " checking:  episode_1727294829\n",
      " checking:  episode_1727294874\n",
      " checking:  episode_1727294887\n",
      " checking:  episode_1727294899\n",
      " checking:  episode_1727294912\n",
      " checking:  episode_1727294926\n",
      " checking:  episode_1727294938\n",
      " checking:  episode_1727294951\n",
      " checking:  episode_1727294962\n",
      " checking:  episode_1727294981\n",
      " checking:  episode_1727294997\n",
      " checking:  episode_1727295009\n",
      " checking:  episode_1727295023\n",
      " checking:  episode_1727295037\n",
      " checking:  episode_1727295050\n",
      " checking:  episode_1727295064\n",
      " checking:  episode_1727295077\n",
      " checking:  episode_1727295088\n",
      " checking:  episode_1727295099\n",
      " checking:  episode_1727295111\n",
      " checking:  episode_1727295123\n",
      " checking:  episode_1727295135\n",
      " checking:  episode_1727295155\n",
      " checking:  episode_1727295169\n",
      " checking:  episode_1727295182\n",
      " checking:  episode_1727295197\n",
      " checking:  episode_1727295208\n",
      " checking:  episode_1727295220\n",
      " checking:  episode_1727295231\n",
      " checking:  episode_1727295241\n",
      " checking:  episode_1727295251\n",
      " checking:  episode_1727295263\n",
      " checking:  episode_1727295274\n",
      " checking:  episode_1727295289\n",
      " checking:  episode_1727295304\n",
      " checking:  episode_1727295316\n",
      " checking:  episode_1727295330\n",
      " checking:  episode_1727295344\n",
      " checking:  episode_1727295356\n",
      " checking:  episode_1727295368\n",
      " checking:  episode_1727295380\n",
      " checking:  episode_1727295393\n",
      " checking:  episode_1727295408\n",
      " checking:  episode_1727295419\n",
      " checking:  episode_1727295430\n",
      " checking:  episode_1727295441\n",
      " checking:  episode_1727295454\n",
      " checking:  episode_1727295471\n",
      " checking:  episode_1727295484\n",
      " checking:  episode_1727295497\n",
      " checking:  episode_1727295511\n",
      " checking:  episode_1727295522\n",
      " checking:  episode_1727295538\n",
      " checking:  episode_1727295552\n",
      " checking:  episode_1727295565\n",
      " checking:  episode_1727295578\n",
      " checking:  episode_1727295591\n",
      " checking:  episode_1727295606\n",
      " checking:  episode_1727295617\n",
      " checking:  episode_1727295651\n",
      " checking:  episode_1727295663\n",
      " checking:  episode_1727295676\n",
      " checking:  episode_1727295688\n",
      "[ReplayBuffer] copying data to memory store.\n",
      "copied to replaybuffer:  143651 0 10020252098\n",
      "[VirtualTargetDataset] raw to obs/action conversion\n",
      "[VirtualTargetDataset] creating SequenceSampler.\n",
      "Test Script: Creating dataloader.\n",
      "train dataset: 13339 train dataloader: 105\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "\n",
    "# # use line-buffering for both stdout and stderr\n",
    "# sys.stdout = open(sys.stdout.fileno(), mode='w', buffering=1)\n",
    "# sys.stderr = open(sys.stderr.fileno(), mode='w', buffering=1)\n",
    "\n",
    "# Add project root to Python path\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.insert(0, ROOT_DIR)\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir(ROOT_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.dataset.base_dataset import BaseImageDataset, BaseDataset\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "\n",
    "# allows arbitrary python code execution in configs using the ${eval:''} resolver\n",
    "OmegaConf.register_new_resolver(\"eval\", eval, replace=True)\n",
    "\n",
    "temp_output_dir = \"./temp_output_dir\"\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "with initialize(\n",
    "    version_base=None,\n",
    "    config_path=str('../diffusion_policy/config'),\n",
    "    job_name=\"test_app\"\n",
    "):\n",
    "    print(\"Test Script: starting.\")\n",
    "    cfg = compose(config_name=\"train_spec_workspace\")\n",
    "    # resolve immediately so all the ${now:} resolvers\n",
    "    # will use the same time.\n",
    "    print(\"Test Script: resolving config.\")\n",
    "    OmegaConf.resolve(cfg)\n",
    "\n",
    "    print(\"Test Script: initializing workspace.\")\n",
    "    cls = hydra.utils.get_class(cfg._target_)\n",
    "    workspace: BaseWorkspace = cls(cfg)\n",
    "    policy = workspace.model\n",
    "\n",
    "    # configure dataset\n",
    "    print(\"Test Script: configuring dataset.\")\n",
    "    dataset: BaseImageDataset\n",
    "    dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "    assert isinstance(dataset, BaseImageDataset) or isinstance(dataset, BaseDataset)\n",
    "    print(\"Test Script: Creating dataloader.\")\n",
    "    train_dataloader = DataLoader(dataset, **cfg.dataloader)\n",
    "    print('train dataset:', len(dataset), 'train dataloader:', len(train_dataloader))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset[5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Script: Computing normalizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset to get normalization: 100%|██████████| 209/209 [00:03<00:00, 60.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_cache_sparse['action'] (213424, 19)\n",
      "Test Script: done\n"
     ]
    }
   ],
   "source": [
    "# compute normalizer on the main process and save to disk\n",
    "print(\"Test Script: Computing normalizer.\")\n",
    "sparse_normalizer_path = os.path.join(temp_output_dir, 'sparse_normalizer.pkl')\n",
    "dense_normalizer_path = os.path.join(temp_output_dir, 'dense_normalizer.pkl')\n",
    "sparse_normalizer = dataset.get_normalizer()\n",
    "policy.set_normalizer(sparse_normalizer)\n",
    "\n",
    "\n",
    "device = policy.device\n",
    "print(\"Test Script: done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweep dataloader parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating dataset with batch_size=16, num_workers=32.:   0%|          | 0/834 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(dataloader, desc=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33miterating dataset with batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, num_workers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_worker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tepoch:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tepoch):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         batch = \u001b[43mdict_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m time.time() - start_time > time_threshold:\n\u001b[32m     29\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m batch_idx / \u001b[38;5;28mlen\u001b[39m(tepoch) < check_ratio:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ACP/adaptive_compliance_policy/PyriteML/diffusion_policy/common/pytorch_util.py:13\u001b[39m, in \u001b[36mdict_apply\u001b[39m\u001b[34m(x, func)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         result[key] = \u001b[43mdict_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     15\u001b[39m         result[key] = func(value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ACP/adaptive_compliance_policy/PyriteML/diffusion_policy/common/pytorch_util.py:13\u001b[39m, in \u001b[36mdict_apply\u001b[39m\u001b[34m(x, func)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         result[key] = \u001b[43mdict_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     15\u001b[39m         result[key] = func(value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ACP/adaptive_compliance_policy/PyriteML/diffusion_policy/common/pytorch_util.py:15\u001b[39m, in \u001b[36mdict_apply\u001b[39m\u001b[34m(x, func)\u001b[39m\n\u001b[32m     13\u001b[39m         result[key] = dict_apply(value, func)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         result[key] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(dataloader, desc=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33miterating dataset with batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, num_workers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_worker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tepoch:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tepoch):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         batch = dict_apply(batch, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m time.time() - start_time > time_threshold:\n\u001b[32m     29\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m batch_idx / \u001b[38;5;28mlen\u001b[39m(tepoch) < check_ratio:\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# sweep batch size and num_workers\n",
    "import time\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "\n",
    "# batch_sizes = [16, 32, 64, 128, 256]\n",
    "# num_workers = [4,8,16,32]\n",
    "batch_sizes = [16]\n",
    "num_workers = [32]\n",
    "timings = {}\n",
    "\n",
    "# 5.5 seconds is the threshold for the time it takes to load 10% of the dataset\n",
    "time_threshold = 20 + 10\n",
    "check_ratio = 0.3\n",
    "device = 'cuda:0'\n",
    "print('starting')\n",
    "print('device:', device)\n",
    "for batch_size in batch_sizes:\n",
    "    for num_worker in num_workers:\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_worker)\n",
    "\n",
    "        start_time = time.time()\n",
    "        # dataset.get_normalizer(batch_size, num_worker)\n",
    "        finished = True\n",
    "        with tqdm(dataloader, desc=f'iterating dataset with batch_size={batch_size}, num_workers={num_worker}.') as tepoch:\n",
    "            for batch_idx, batch in enumerate(tepoch):\n",
    "                batch = dict_apply(batch, lambda x: x.to(device, non_blocking=True))\n",
    "                if time.time() - start_time > time_threshold:\n",
    "                    if batch_idx / len(tepoch) < check_ratio:\n",
    "                        print(f\"Time exceeded threshold of {time_threshold} seconds, but data loaded is less than {check_ratio}. Exiting.\")\n",
    "                        finished = False\n",
    "                        break\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        if finished:\n",
    "            timings[(batch_size, num_worker)] = elapsed_time\n",
    "\n",
    "fastest_batch_size = min(timings, key=timings.get)\n",
    "print(\"Fastest batch size:\", fastest_batch_size, \", Fastest time:\", timings[fastest_batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read a batch of data, test normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['obs', 'action'])\n",
      "   obs.sparse.key:  rgb_0 torch.Size([128, 2, 3, 224, 224])\n",
      "   obs.sparse.key:  robot0_eef_pos torch.Size([128, 3, 3])\n",
      "   obs.sparse.key:  robot0_eef_rot_axis_angle torch.Size([128, 3, 6])\n",
      "   obs.sparse.key:  robot0_eef_wrench torch.Size([128, 7000, 6])\n",
      "policy debug: batch['action']['sparse'][0,0,:]:  tensor([ 0.0000e+00, -2.9802e-08, -1.4901e-08,  1.0000e+00,  3.4119e-08,\n",
      "        -5.3585e-08,  3.4119e-08,  1.0000e+00,  3.5782e-08,  0.0000e+00,\n",
      "        -2.9802e-08, -1.4901e-08,  1.0000e+00,  8.4664e-06, -1.0927e-06,\n",
      "        -8.4497e-06,  1.0000e+00,  9.7696e-06,  5.0000e+03])\n",
      "policy debug: nactions_sparse[0,0,:]:  tensor([ 1.3410e-01, -2.0990e-01,  1.9732e-01,  1.0000e+00,  3.4119e-08,\n",
      "        -5.3585e-08,  3.4119e-08,  1.0000e+00,  3.5782e-08,  1.1233e-01,\n",
      "        -1.4039e-01,  1.9732e-01,  1.0000e+00,  8.4664e-06, -1.0927e-06,\n",
      "        -8.4497e-06,  1.0000e+00,  9.7696e-06,  1.0000e+00],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reach one batch of data from the dataloader\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "print(batch.keys())\n",
    "for key, attr in batch['obs']['sparse'].items():\n",
    "    print(\"   obs.sparse.key: \", key, attr.shape)\n",
    "#for key, attr in batch['obs']['dense'].items():\n",
    "    #print(\"   obs.dense.key: \", key, attr.shape)\n",
    "\n",
    "nobs_sparse = sparse_normalizer.normalize(batch['obs']['sparse'])\n",
    "nactions_sparse = sparse_normalizer['action'].normalize(batch['action']['sparse'])\n",
    "# nactions_dense = dense_normalizer['action'].normalize(batch['action']['dense'])\n",
    "\n",
    "\n",
    "\n",
    "# test normalizer\n",
    "print(\"policy debug: batch['action']['sparse'][0,0,:]: \", batch['action']['sparse'][0,0,:])\n",
    "print(\"policy debug: nactions_sparse[0,0,:]: \", nactions_sparse[0,0,:])\n",
    "\n",
    "# batch = dict_apply(batch, lambda x: x.to(device, non_blocking=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test compute_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1602, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flag = {\n",
    "    'start_training_dense': True,\n",
    "    'dense_traj_cond_use_gt': True\n",
    "}\n",
    "raw_loss = policy(batch, flag)\n",
    "print(raw_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "def log_action_mse(step_log, category, pred_action, gt_action):\n",
    "    B, T, _ = pred_action['sparse'].shape\n",
    "    pred_action_sparse = pred_action['sparse'].view(B, T, -1, 19)\n",
    "    gt_action_sparse = gt_action['sparse'].view(B, T, -1, 19)\n",
    "    step_log[f'{category}_sparse_action_mse_error'] = torch.nn.functional.mse_loss(pred_action_sparse, gt_action_sparse)\n",
    "    step_log[f'{category}_sparse_action_mse_error_pos'] = torch.nn.functional.mse_loss(pred_action_sparse[..., :3], gt_action_sparse[..., :3])\n",
    "    step_log[f'{category}_sparse_action_mse_error_rot'] = torch.nn.functional.mse_loss(pred_action_sparse[..., 3:9], gt_action_sparse[..., 3:9])\n",
    "    # step_log[f'{category}_sparse_action_mse_error_width'] = torch.nn.functional.mse_loss(pred_action_sparse[..., 9], gt_action_sparse[..., 9])\n",
    "    #B, T, _, _= pred_action['dense'].shape\n",
    "    #pred_action_dense = pred_action['dense'].view(B, T, -1, 9)\n",
    "    #gt_action_dense = gt_action['dense'].view(B, T, -1, 9)\n",
    "    #step_log[f'{category}_dense_action_mse_error'] = torch.nn.functional.mse_loss(pred_action_dense, gt_action_dense)\n",
    "    #step_log[f'{category}_dense_action_mse_error_pos'] = torch.nn.functional.mse_loss(pred_action_dense[..., :3], gt_action_dense[..., :3])\n",
    "    #step_log[f'{category}_dense_action_mse_error_rot'] = torch.nn.functional.mse_loss(pred_action_dense[..., 3:9], gt_action_dense[..., 3:9])\n",
    "    # step_log[f'{category}_dense_action_mse_error_width'] = torch.nn.functional.mse_loss(pred_action_dense[..., 9], gt_action_dense[..., 9])\n",
    "                \n",
    "# sample trajectory from training set, and evaluate difference\n",
    "gt_action = batch['action']\n",
    "pred_action = policy.predict_action(batch['obs'])\n",
    "print(\"gt_action['sparse'].shape: \", gt_action['sparse'].shape)\n",
    "print(\"pred_action['sparse'].shape: \", pred_action['sparse'].shape)\n",
    "#print(\"gt_action['dense'].shape: \", gt_action['dense'].shape)\n",
    "#print(\"pred_action['dense'].shape: \", pred_action['dense'].shape)\n",
    "\n",
    "step_log = {}\n",
    "log_action_mse(step_log, 'train', pred_action, gt_action)\n",
    "print(step_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
