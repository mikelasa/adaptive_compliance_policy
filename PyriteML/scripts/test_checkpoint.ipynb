{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import dill\n",
    "import hydra\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.dataset.base_dataset import BaseImageDataset, BaseDataset\n",
    "from diffusion_policy.workspace.train_diffusion_unet_image_workspace import TrainDiffusionUnetImageWorkspace\n",
    "\n",
    "data_path = \"/training_outputs/\"\n",
    "ckpt_path = data_path + \"2024.05.21_22.06.13_flip_up_linear_interpolated_dense_action/checkpoints/epoch=0040-train_loss=0.012.ckpt\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# load checkpoint\n",
    "if not ckpt_path.endswith('.ckpt'):\n",
    "    ckpt_path = os.path.join(ckpt_path, 'checkpoints', 'latest.ckpt')\n",
    "payload = torch.load(open(ckpt_path, 'rb'), map_location='cpu', pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "print(\"model_name:\", cfg.policy.obs_encoder.model_name)\n",
    "print(\"dataset_path:\", cfg.task.dataset.dataset_path)\n",
    "\n",
    "cls = hydra.utils.get_class(cfg._target_)\n",
    "workspace = cls(cfg)\n",
    "workspace: BaseWorkspace\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n",
    "policy = workspace.model\n",
    "if cfg.training.use_ema:\n",
    "    policy = workspace.ema_model\n",
    "policy.num_inference_steps = cfg.policy.num_inference_steps # DDIM inference iterations\n",
    "\n",
    "policy.eval().to(device)\n",
    "policy.reset()\n",
    "\n",
    "# use normalizer saved in the policy\n",
    "sparse_normalizer, dense_normalizer = policy.get_normalizer()\n",
    "\n",
    "shape_meta = cfg.task.shape_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the dataset used in training\n",
    "# dataset: BaseImageDataset\n",
    "# dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "# assert isinstance(dataset, BaseImageDataset) or isinstance(dataset, BaseDataset)\n",
    "# print(\"Test Script: Creating dataloader.\")\n",
    "# train_dataloader = DataLoader(dataset, **cfg.dataloader)\n",
    "# print('train dataset:', len(dataset), 'train dataloader:', len(train_dataloader))\n",
    "\n",
    "# load the dataset specified in config\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize(\n",
    "    version_base=None,\n",
    "    config_path=str('../diffusion_policy/config'),\n",
    "    job_name=\"test_app\"\n",
    "):\n",
    "    cfg = compose(config_name=\"train_diffusion_unet_timm_flip_up_workspace\")\n",
    "    OmegaConf.resolve(cfg)\n",
    "\n",
    "    print(\"Test Script: configuring dataset.\")\n",
    "    dataset: BaseImageDataset\n",
    "    dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "    assert isinstance(dataset, BaseImageDataset) or isinstance(dataset, BaseDataset)\n",
    "    print(\"Test Script: Creating dataloader.\")\n",
    "    train_dataloader = DataLoader(dataset, **cfg.dataloader)\n",
    "    print('train dataset:', len(dataset), 'train dataloader:', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from einops import rearrange, reduce\n",
    "import json\n",
    "\n",
    "def log_action_mse(step_log, category, pred_action, gt_action):\n",
    "    pred_naction = {\n",
    "        'sparse': sparse_normalizer['action'].normalize(pred_action['sparse']),\n",
    "        'dense': dense_normalizer['action'].normalize(pred_action['dense'])\n",
    "    }\n",
    "    gt_naction = {\n",
    "        'sparse': sparse_normalizer['action'].normalize(gt_action['sparse']),\n",
    "        'dense': dense_normalizer['action'].normalize(gt_action['dense'])\n",
    "    }\n",
    "\n",
    "    B, T, _ = pred_naction['sparse'].shape\n",
    "    pred_naction_sparse = pred_naction['sparse'].view(B, T, -1, 9)\n",
    "    gt_naction_sparse = gt_naction['sparse'].view(B, T, -1, 9)\n",
    "    sparse_loss = F.mse_loss(pred_naction_sparse, gt_naction_sparse, reduction='none')\n",
    "    sparse_loss = sparse_loss.type(sparse_loss.dtype)\n",
    "    sparse_loss = reduce(sparse_loss, 'b ... -> b (...)', 'mean')\n",
    "    sparse_loss = sparse_loss.mean()            \n",
    "\n",
    "    step_log[f'{category}_sparse_naction_mse_error'] = float(sparse_loss.detach())\n",
    "    # step_log[f'{category}_sparse_naction_mse_error_pos'] = F.mse_loss(pred_naction_sparse[..., :3], gt_naction_sparse[..., :3])\n",
    "    # step_log[f'{category}_sparse_naction_mse_error_rot'] = F.mse_loss(pred_naction_sparse[..., 3:9], gt_naction_sparse[..., 3:9])\n",
    "    B, T, _, _= pred_naction['dense'].shape\n",
    "    pred_naction_dense = pred_naction['dense'].view(B, T, -1, 9)\n",
    "    gt_naction_dense = gt_naction['dense'].view(B, T, -1, 9)\n",
    "    dense_loss = F.mse_loss(pred_naction_dense, gt_naction_dense, reduction='none')\n",
    "    dense_loss = dense_loss.type(dense_loss.dtype)\n",
    "    dense_loss = reduce(dense_loss, 'b ... -> b (...)', 'mean')\n",
    "    dense_loss = dense_loss.mean()            \n",
    "    step_log[f'{category}_dense_naction_mse_error'] = float(dense_loss.detach())\n",
    "    # step_log[f'{category}_dense_naction_mse_error_pos'] = F.mse_loss(pred_naction_dense[..., :3], gt_naction_dense[..., :3])\n",
    "    # step_log[f'{category}_dense_naction_mse_error_rot'] = F.mse_loss(pred_naction_dense[..., 3:9], gt_naction_dense[..., 3:9])\n",
    "    \n",
    "# get a batch of data'\n",
    "print('get a batch of data')\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# print(batch.keys())\n",
    "# for key, attr in batch['obs']['sparse'].items():\n",
    "#     print(\"   obs.sparse.key: \", key, attr.shape)\n",
    "# for key, attr in batch['obs']['dense'].items():\n",
    "#     print(\"   obs.dense.key: \", key, attr.shape)\n",
    "# for key, attr in batch['action'].items():\n",
    "#     print(\"   action.key: \", key, attr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test compute loss\n",
    "print('running policy on batch')\n",
    "flag = {'dense_traj_cond_use_gt': True}\n",
    "raw_loss = policy(batch, flag)\n",
    "print('total loss: ', raw_loss)\n",
    "print('sparse loss:', policy.sparse_loss)\n",
    "print('dense loss:', policy.dense_loss)\n",
    "\n",
    "# test predict action\n",
    "gt_action = batch['action']\n",
    "pred_action = policy.predict_action(batch['obs'], batch['action']) # providing batch will enable gt sparse condition\n",
    "# print(\"gt_action['sparse'].shape: \", gt_action['sparse'].shape)\n",
    "# print(\"pred_action['sparse'].shape: \", pred_action['sparse'].shape)\n",
    "# print(\"gt_action['dense'].shape: \", gt_action['dense'].shape)\n",
    "# print(\"pred_action['dense'].shape: \", pred_action['dense'].shape)\n",
    "\n",
    "step_log = {}\n",
    "log_action_mse(step_log, 'train', pred_action, gt_action)\n",
    "print(json.dumps(step_log, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
