{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/robotlab/ACP/adaptive_compliance_policy/PyriteML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robotlab/micromamba/envs/pyrite/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config keys: dict_keys(['name', 'output_dir', '_target_', 'task_name', 'shape_meta', 'exp_name', 'policy', 'ema', 'dataloader', 'val_dataloader', 'optimizer', 'training', 'logging', 'checkpoint', 'multi_run', 'task'])\n",
      "\n",
      "Dataset path: /home/robotlab/ACP/data/real_processed/wipe_single_arm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robotlab/micromamba/envs/pyrite/lib/python3.12/site-packages/torch/nn/init.py:566: UserWarning:\n",
      "\n",
      "Initializing zero-element tensors is a no-op\n",
      "\n",
      "vit will use the CLS token. feature_aggregation (attention_pool_2d) is ignored!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb keys:          ['rgb_0']\n",
      "low_dim_keys keys: ['robot0_eef_pos', 'robot0_eef_rot_axis_angle']\n",
      "==> reduce pretrained obs_encorder's lr\n",
      "==> rgb keys:  ['rgb_0']\n",
      "obs_encorder params: 151\n",
      "\n",
      "Normalizer keys: ['action', 'rgb_0', 'robot0_eef_pos', 'robot0_eef_rot_axis_angle', 'robot0_eef_wrench']\n",
      "\n",
      "Loaded checkpoint successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "\n",
    "# Add project root to Python path\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.insert(0, ROOT_DIR)\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir(ROOT_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import dill\n",
    "import hydra\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.dataset.base_dataset import BaseImageDataset, BaseDataset\n",
    "from diffusion_policy.workspace.train_diffusion_unet_image_workspace import TrainDiffusionUnetImageWorkspace\n",
    "\n",
    "data_path = \"/home/robotlab/ACP/training_outputs/\"\n",
    "ckpt_path = data_path + \"2025.11.04_10.05.05_Wipe_single_arm_Wipe_single_arm/checkpoints/latest.ckpt\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# load checkpoint\n",
    "if not ckpt_path.endswith('.ckpt'):\n",
    "    ckpt_path = os.path.join(ckpt_path, 'checkpoints', 'latest.ckpt')\n",
    "payload = torch.load(open(ckpt_path, 'rb'), map_location='cpu', pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "\n",
    "# Inspect config structure\n",
    "print(\"Config keys:\", cfg.keys())\n",
    "print(\"\\nDataset path:\", cfg.task.dataset.dataset_path if 'task' in cfg else \"N/A\")\n",
    "\n",
    "cls = hydra.utils.get_class(cfg._target_)\n",
    "workspace = cls(cfg)\n",
    "workspace: BaseWorkspace\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n",
    "policy = workspace.model\n",
    "if cfg.training.use_ema:\n",
    "    policy = workspace.ema_model\n",
    "policy.num_inference_steps = cfg.policy.num_inference_steps\n",
    "\n",
    "policy.eval().to(device)\n",
    "policy.reset()\n",
    "\n",
    "# Get the single normalizer (handles both sparse and dense)\n",
    "normalizer = policy.get_normalizer()\n",
    "print(f\"\\nNormalizer keys: {list(normalizer.params_dict.keys())}\")\n",
    "\n",
    "shape_meta = cfg.task.shape_meta\n",
    "print(f\"\\nLoaded checkpoint successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Script: configuring dataset.\n",
      "[VirtualTargetDataset] loading data into store\n",
      "[ReplayBuffer] checking chunk size and compressor.\n",
      " checking:  episode_1727235551\n",
      " checking:  episode_1727235569\n",
      " checking:  episode_1727235583\n",
      " checking:  episode_1727235597\n",
      " checking:  episode_1727235613\n",
      " checking:  episode_1727235630\n",
      " checking:  episode_1727235645\n",
      " checking:  episode_1727235661\n",
      " checking:  episode_1727235681\n",
      " checking:  episode_1727235696\n",
      " checking:  episode_1727235710\n",
      " checking:  episode_1727235725\n",
      " checking:  episode_1727235744\n",
      " checking:  episode_1727235765\n",
      " checking:  episode_1727235779\n",
      " checking:  episode_1727235795\n",
      " checking:  episode_1727235810\n",
      " checking:  episode_1727235825\n",
      " checking:  episode_1727235839\n",
      " checking:  episode_1727236158\n",
      " checking:  episode_1727236172\n",
      " checking:  episode_1727236186\n",
      " checking:  episode_1727236202\n",
      " checking:  episode_1727236216\n",
      " checking:  episode_1727236238\n",
      " checking:  episode_1727236253\n",
      " checking:  episode_1727236265\n",
      " checking:  episode_1727236280\n",
      " checking:  episode_1727236293\n",
      " checking:  episode_1727236306\n",
      " checking:  episode_1727236422\n",
      " checking:  episode_1727236437\n",
      " checking:  episode_1727236453\n",
      " checking:  episode_1727236470\n",
      " checking:  episode_1727236484\n",
      " checking:  episode_1727236501\n",
      " checking:  episode_1727236518\n",
      " checking:  episode_1727236530\n",
      " checking:  episode_1727236543\n",
      " checking:  episode_1727236557\n",
      " checking:  episode_1727236577\n",
      " checking:  episode_1727236590\n",
      " checking:  episode_1727236603\n",
      " checking:  episode_1727236622\n",
      " checking:  episode_1727236635\n",
      " checking:  episode_1727236651\n",
      " checking:  episode_1727236663\n",
      " checking:  episode_1727236709\n",
      " checking:  episode_1727236724\n",
      " checking:  episode_1727236737\n",
      " checking:  episode_1727236774\n",
      " checking:  episode_1727236787\n",
      " checking:  episode_1727236800\n",
      " checking:  episode_1727236812\n",
      " checking:  episode_1727236825\n",
      " checking:  episode_1727236841\n",
      " checking:  episode_1727236854\n",
      " checking:  episode_1727236867\n",
      " checking:  episode_1727236878\n",
      " checking:  episode_1727236892\n",
      " checking:  episode_1727236906\n",
      " checking:  episode_1727236918\n",
      " checking:  episode_1727236930\n",
      " checking:  episode_1727236942\n",
      " checking:  episode_1727236957\n",
      " checking:  episode_1727236970\n",
      " checking:  episode_1727236984\n",
      " checking:  episode_1727236998\n",
      " checking:  episode_1727237012\n",
      " checking:  episode_1727237025\n",
      " checking:  episode_1727237040\n",
      " checking:  episode_1727237053\n",
      " checking:  episode_1727237065\n",
      " checking:  episode_1727237083\n",
      " checking:  episode_1727237119\n",
      " checking:  episode_1727237131\n",
      " checking:  episode_1727237142\n",
      " checking:  episode_1727237156\n",
      " checking:  episode_1727237166\n",
      " checking:  episode_1727237179\n",
      " checking:  episode_1727237196\n",
      " checking:  episode_1727237213\n",
      " checking:  episode_1727237225\n",
      " checking:  episode_1727237242\n",
      " checking:  episode_1727237253\n",
      " checking:  episode_1727237265\n",
      " checking:  episode_1727237282\n",
      " checking:  episode_1727237292\n",
      " checking:  episode_1727237304\n",
      " checking:  episode_1727237318\n",
      " checking:  episode_1727237402\n",
      " checking:  episode_1727237414\n",
      " checking:  episode_1727237432\n",
      " checking:  episode_1727237445\n",
      " checking:  episode_1727237458\n",
      " checking:  episode_1727237490\n",
      " checking:  episode_1727237505\n",
      " checking:  episode_1727237523\n",
      " checking:  episode_1727237539\n",
      " checking:  episode_1727237550\n",
      " checking:  episode_1727237561\n",
      " checking:  episode_1727237574\n",
      " checking:  episode_1727237590\n",
      " checking:  episode_1727237602\n",
      " checking:  episode_1727237621\n",
      " checking:  episode_1727237631\n",
      " checking:  episode_1727237642\n",
      " checking:  episode_1727237654\n",
      " checking:  episode_1727237666\n",
      " checking:  episode_1727237681\n",
      " checking:  episode_1727237695\n",
      " checking:  episode_1727237709\n",
      " checking:  episode_1727237723\n",
      " checking:  episode_1727237744\n",
      " checking:  episode_1727237757\n",
      " checking:  episode_1727237774\n",
      " checking:  episode_1727237786\n",
      " checking:  episode_1727237797\n",
      " checking:  episode_1727237815\n",
      " checking:  episode_1727237826\n",
      " checking:  episode_1727237841\n",
      " checking:  episode_1727237853\n",
      " checking:  episode_1727237873\n",
      " checking:  episode_1727237886\n",
      " checking:  episode_1727237900\n",
      " checking:  episode_1727237913\n",
      " checking:  episode_1727237928\n",
      " checking:  episode_1727237942\n",
      " checking:  episode_1727237956\n",
      " checking:  episode_1727237991\n",
      " checking:  episode_1727294154\n",
      " checking:  episode_1727294262\n",
      " checking:  episode_1727294278\n",
      " checking:  episode_1727294290\n",
      " checking:  episode_1727294303\n",
      " checking:  episode_1727294314\n",
      " checking:  episode_1727294326\n",
      " checking:  episode_1727294341\n",
      " checking:  episode_1727294351\n",
      " checking:  episode_1727294366\n",
      " checking:  episode_1727294378\n",
      " checking:  episode_1727294389\n",
      " checking:  episode_1727294407\n",
      " checking:  episode_1727294417\n",
      " checking:  episode_1727294432\n",
      " checking:  episode_1727294464\n",
      " checking:  episode_1727294482\n",
      " checking:  episode_1727294501\n",
      " checking:  episode_1727294515\n",
      " checking:  episode_1727294531\n",
      " checking:  episode_1727294545\n",
      " checking:  episode_1727294558\n",
      " checking:  episode_1727294573\n",
      " checking:  episode_1727294596\n",
      " checking:  episode_1727294610\n",
      " checking:  episode_1727294624\n",
      " checking:  episode_1727294639\n",
      " checking:  episode_1727294652\n",
      " checking:  episode_1727294668\n",
      " checking:  episode_1727294683\n",
      " checking:  episode_1727294696\n",
      " checking:  episode_1727294709\n",
      " checking:  episode_1727294749\n",
      " checking:  episode_1727294765\n",
      " checking:  episode_1727294782\n",
      " checking:  episode_1727294804\n",
      " checking:  episode_1727294817\n",
      " checking:  episode_1727294829\n",
      " checking:  episode_1727294874\n",
      " checking:  episode_1727294887\n",
      " checking:  episode_1727294899\n",
      " checking:  episode_1727294912\n",
      " checking:  episode_1727294926\n",
      " checking:  episode_1727294938\n",
      " checking:  episode_1727294951\n",
      " checking:  episode_1727294962\n",
      " checking:  episode_1727294981\n",
      " checking:  episode_1727294997\n",
      " checking:  episode_1727295009\n",
      " checking:  episode_1727295023\n",
      " checking:  episode_1727295037\n",
      " checking:  episode_1727295050\n",
      " checking:  episode_1727295064\n",
      " checking:  episode_1727295077\n",
      " checking:  episode_1727295088\n",
      " checking:  episode_1727295099\n",
      " checking:  episode_1727295111\n",
      " checking:  episode_1727295123\n",
      " checking:  episode_1727295135\n",
      " checking:  episode_1727295155\n",
      " checking:  episode_1727295169\n",
      " checking:  episode_1727295182\n",
      " checking:  episode_1727295197\n",
      " checking:  episode_1727295208\n",
      " checking:  episode_1727295220\n",
      " checking:  episode_1727295231\n",
      " checking:  episode_1727295241\n",
      " checking:  episode_1727295251\n",
      " checking:  episode_1727295263\n",
      " checking:  episode_1727295274\n",
      " checking:  episode_1727295289\n",
      " checking:  episode_1727295304\n",
      " checking:  episode_1727295316\n",
      " checking:  episode_1727295330\n",
      " checking:  episode_1727295344\n",
      " checking:  episode_1727295356\n",
      " checking:  episode_1727295368\n",
      " checking:  episode_1727295380\n",
      " checking:  episode_1727295393\n",
      " checking:  episode_1727295408\n",
      " checking:  episode_1727295419\n",
      " checking:  episode_1727295430\n",
      " checking:  episode_1727295441\n",
      " checking:  episode_1727295454\n",
      " checking:  episode_1727295471\n",
      " checking:  episode_1727295484\n",
      " checking:  episode_1727295497\n",
      " checking:  episode_1727295511\n",
      " checking:  episode_1727295522\n",
      " checking:  episode_1727295538\n",
      " checking:  episode_1727295552\n",
      " checking:  episode_1727295565\n",
      " checking:  episode_1727295578\n",
      " checking:  episode_1727295591\n",
      " checking:  episode_1727295606\n",
      " checking:  episode_1727295617\n",
      " checking:  episode_1727295651\n",
      " checking:  episode_1727295663\n",
      " checking:  episode_1727295676\n",
      " checking:  episode_1727295688\n",
      "[ReplayBuffer] copying data to memory store.\n",
      "copied to replaybuffer:  143651 0 10020252098\n",
      "[VirtualTargetDataset] raw to obs/action conversion\n",
      "[VirtualTargetDataset] creating SequenceSampler.\n",
      "Test Script: Creating dataloader.\n",
      "train dataset: 13339 train dataloader: 105\n"
     ]
    }
   ],
   "source": [
    "# # load the dataset used in training\n",
    "# dataset: BaseImageDataset\n",
    "# dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "# assert isinstance(dataset, BaseImageDataset) or isinstance(dataset, BaseDataset)\n",
    "# print(\"Test Script: Creating dataloader.\")\n",
    "# train_dataloader = DataLoader(dataset, **cfg.dataloader)\n",
    "# print('train dataset:', len(dataset), 'train dataloader:', len(train_dataloader))\n",
    "\n",
    "# load the dataset specified in config\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize(\n",
    "    version_base=None,\n",
    "    config_path=str('../diffusion_policy/config'),\n",
    "    job_name=\"test_app\"\n",
    "):\n",
    "    cfg = compose(config_name=\"train_spec_workspace\")\n",
    "    OmegaConf.resolve(cfg)\n",
    "\n",
    "    print(\"Test Script: configuring dataset.\")\n",
    "    dataset: BaseImageDataset\n",
    "    dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "    assert isinstance(dataset, BaseImageDataset) or isinstance(dataset, BaseDataset)\n",
    "    print(\"Test Script: Creating dataloader.\")\n",
    "    train_dataloader = DataLoader(dataset, **cfg.dataloader)\n",
    "    print('train dataset:', len(dataset), 'train dataloader:', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get a batch of data\n",
      "pred_action keys: not computed yet\n",
      "gt_action keys: dict_keys(['sparse'])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from einops import rearrange, reduce\n",
    "import json\n",
    "\n",
    "def log_action_mse(step_log, category, pred_action, gt_action):\n",
    "    # Only process keys that exist in both pred and gt\n",
    "    for key in ['sparse', 'dense']:\n",
    "        if key not in pred_action or key not in gt_action:\n",
    "            continue\n",
    "            \n",
    "        pred_naction = normalizer['action'].normalize(pred_action[key])\n",
    "        gt_naction = normalizer['action'].normalize(gt_action[key])\n",
    "        \n",
    "        B, T, _ = pred_naction.shape\n",
    "        pred_naction = pred_naction.view(B, T, -1, 19)  # 19 action dims\n",
    "        gt_naction = gt_naction.view(B, T, -1, 19)\n",
    "        \n",
    "        loss = F.mse_loss(pred_naction, gt_naction, reduction='none')\n",
    "        loss = loss.type(loss.dtype)\n",
    "        loss = reduce(loss, 'b ... -> b (...)', 'mean')\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        step_log[f'{category}_{key}_naction_mse_error'] = float(loss.detach())\n",
    "    \n",
    "# get a batch of data\n",
    "print('get a batch of data')\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# Inspect what keys are actually present\n",
    "print(\"pred_action keys:\", pred_action.keys() if 'pred_action' in locals() else \"not computed yet\")\n",
    "print(\"gt_action keys:\", batch['action'].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running policy on batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss:  tensor(0.1973, grad_fn=<MseLossBackward0>)\n",
      "sparse loss: tensor(0.1973, grad_fn=<MseLossBackward0>)\n",
      "gt_action['sparse'].shape:  torch.Size([128, 16, 19])\n",
      "pred_action['sparse'].shape:  torch.Size([128, 16, 19])\n",
      "{\n",
      "    \"train_sparse_naction_mse_error\": 0.27118033170700073\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test compute loss\n",
    "print('running policy on batch')\n",
    "flag = {'dense_traj_cond_use_gt': True}\n",
    "raw_loss = policy(batch, flag)\n",
    "print('total loss: ', raw_loss)\n",
    "print('sparse loss:', policy.sparse_loss)\n",
    "#print('dense loss:', policy.dense_loss)\n",
    "\n",
    "# test predict action\n",
    "gt_action = batch['action']\n",
    "pred_action = policy.predict_action(batch['obs']) # providing batch will enable gt sparse condition\n",
    "print(\"gt_action['sparse'].shape: \", gt_action['sparse'].shape)\n",
    "print(\"pred_action['sparse'].shape: \", pred_action['sparse'].shape)\n",
    "#print(\"gt_action['dense'].shape: \", gt_action['dense'].shape)\n",
    "#print(\"pred_action['dense'].shape: \", pred_action['dense'].shape)\n",
    "\n",
    "step_log = {}\n",
    "log_action_mse(step_log, 'train', pred_action, gt_action)\n",
    "print(json.dumps(step_log, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
